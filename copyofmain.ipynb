{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf451c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Path to your SQLite DB file\n",
    "db_path = \"data/grocery_sales.db\"\n",
    "db_name = os.path.splitext(os.path.basename(db_path))[0]\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get all user-defined tables\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT name FROM sqlite_master\n",
    "    WHERE type='table' AND name NOT LIKE 'sqlite_%';\n",
    "\"\"\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Build schema dictionary with column names and data types\n",
    "schema_info = {}\n",
    "\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    cursor.execute(f\"PRAGMA table_info('{table_name}')\")\n",
    "    columns = cursor.fetchall()\n",
    "    column_info = {col[1]: col[2] for col in columns}  # {column_name: data_type}\n",
    "    schema_info[table_name] = column_info\n",
    "\n",
    "# Convert to JSON\n",
    "schema_json = json.dumps(schema_info, indent=2)\n",
    "\n",
    "json_path = db_name + \".json\"\n",
    "\n",
    "# Optional: write to file\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    f.write(schema_json)\n",
    "\n",
    "# Cleanup\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Schema exported to JSON with column names and data types.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccaf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Extracts the first JSON-like object from a text response using regex.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\{[\\s\\S]*\\}\", response_text)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response.\")\n",
    "    return match.group(0)\n",
    "\n",
    "def generate_column_aliases_llm(schema_dict, llm, cache_path):\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    all_columns = set(col for cols in schema_dict.values() for col in cols)\n",
    "    schema_str = \", \".join(all_columns)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a data expert helping to identify common abbreviations or typos for column names in a database.\n",
    "    You can give multiple abbreviations for each column name so we can use it in a Question-Answer chatbot to correct user input.\n",
    "\n",
    "    Given this list of column names: [{schema_str}],\n",
    "    Generate a JSON mapping where each key is the correct column name and the value is a list of likely short forms, typos, or natural-language variants.\n",
    "\n",
    "    Example format:\n",
    "    {{\n",
    "      \"CountryName\": [\"cnt\", \"con\", \"ctry\"],\n",
    "      \"CustomerID\": [\"custid\", \"cid\", \"cstm\"]\n",
    "    }}\n",
    "\n",
    "    Return only valid JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    result = llm.invoke(prompt)\n",
    "    response_text = result.content.strip()\n",
    "\n",
    "    try:\n",
    "        json_str = extract_json_from_response(response_text)\n",
    "        \n",
    "        # Use ast.literal_eval to handle non-strict JSON like single quotes\n",
    "        parsed = ast.literal_eval(json_str)\n",
    "\n",
    "        # Convert to strict JSON-compatible dict\n",
    "        alias_dict = json.loads(json.dumps(parsed))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from LLM response:\")\n",
    "        print(response_text)\n",
    "        raise e\n",
    "\n",
    "    with open(cache_path, \"w\") as f:\n",
    "        json.dump(alias_dict, f, indent=2)\n",
    "\n",
    "    return alias_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "groq_api_key = \"Non dissclosable"\n",
    "groq_api_base = \"https://api.groq.com/openai/v1\"\n",
    "model_name = \"llama3-70b-8192\"\n",
    "llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "    temperature=0.0,\n",
    "    openai_api_key=groq_api_key,\n",
    "    openai_api_base=groq_api_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the 'grocery_sales.json' file in read mode\n",
    "with open('grocery_sales.json', 'r') as file:\n",
    "    # Loading the JSON content into a Python dictionary\n",
    "    schema_grocery_sales = json.load(file)\n",
    "\n",
    "# Setting the path to store the generated column aliases\n",
    "cache_path = \"grocery_alias.json\"\n",
    "\n",
    "# Generating column aliases using the LLM and saving them to the cache file\n",
    "generate_column_aliases_llm(schema_grocery_sales, llm, cache_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2828709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for file handling, serialization, vector search, and embeddings\n",
    "import os\n",
    "import pickle\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Setting constants for the schema index directory and embedding model to use\n",
    "SCHEMA_INDEX_DIR = \"schema_index\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Build or Load Schema Embeddings\n",
    "# -----------------------------------------------\n",
    "\n",
    "def build_schema_index(schema_dict, save_path=SCHEMA_INDEX_DIR):\n",
    "    # Initializing the Hugging Face sentence embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    \n",
    "    # Flattening the schema into \"table.column\" format for embedding\n",
    "    schema_lines = [f\"{table}.{col}\" for table, cols in schema_dict.items() for col in cols]\n",
    "    # Wrapping each schema line into a Document object\n",
    "    documents = [Document(page_content=line) for line in schema_lines]\n",
    "\n",
    "    # Creating a FAISS vector index from the embedded documents\n",
    "    db = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "    # Creating the directory if it doesn't exist and saving the FAISS index\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    db.save_local(folder_path=save_path)\n",
    "    print(f\"Schema index saved to: {save_path}\")\n",
    "\n",
    "def load_schema_index(load_path=SCHEMA_INDEX_DIR):\n",
    "    # Initializing the same embedding model to match the one used in indexing\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    # Loading the FAISS index from the local directory\n",
    "    return FAISS.load_local(\n",
    "        folder_path=load_path,\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True  # Needed if using pickle under the hood\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Query Similar Schema Entries\n",
    "# -----------------------------------------------\n",
    "\n",
    "def search_schema(index, query, top_k=5):\n",
    "    # Performing a similarity search on the index for the given query\n",
    "    results = index.similarity_search(query, k=top_k)\n",
    "    # Returning the matched schema entries as plain text\n",
    "    return [doc.page_content for doc in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-time build (comment this after first run)\n",
    "build_schema_index(schema_grocery_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a36919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the previously saved FAISS schema index from local storage\n",
    "index = load_schema_index()\n",
    "\n",
    "# Defining a natural language query to search the schema\n",
    "query = \"Which product has the highest sales\"\n",
    "\n",
    "# Searching the index for schema entries most similar to the query\n",
    "matches = search_schema(index, query)\n",
    "\n",
    "# Printing out the top matching columns based on semantic similarity\n",
    "print(\"🔍 Top matching columns:\")\n",
    "for match in matches:\n",
    "    print(\"-\", match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4813ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def llm_correct_prompt(query: str, alias_dict: dict, llm) -> str:\n",
    "    \"\"\"\n",
    "    Uses LLM to correct typos/shortforms in the query using alias_dict/schema.\n",
    "    Returns the corrected query string only.\n",
    "    \"\"\"\n",
    "\n",
    "    db_schema_str = json.dumps(alias_dict, indent=2)\n",
    "\n",
    "    prompt_temp = PromptTemplate.from_template(\"\"\"\n",
    "You are a Data Assistant and SQL expert. Your task is to correct user prompts for spelling mistakes or shortforms based on the database schema.\n",
    "\n",
    "The schema is in the format:\n",
    "\"Table name\": {{\n",
    "  \"Column name\": \"datatype\",\n",
    "  \"Column name\": \"datatype\"    \n",
    "}}\n",
    "\n",
    "User prompt:\n",
    "{query}\n",
    "\n",
    "Database schema:\n",
    "{db_schema}\n",
    "\n",
    "Return your response in **only** this JSON format:\n",
    "{{ \"corrected_query\": \"...\" }}\n",
    "\"\"\")\n",
    "\n",
    "    prompt = prompt_temp.format(query=query, db_schema=db_schema_str)\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    raw_output = response.content.strip()\n",
    "    print(\"🔍 Raw LLM Output:\\n\", raw_output)\n",
    "\n",
    "    try:\n",
    "        # Remove markdown-style code block if present\n",
    "        cleaned_output = re.sub(r\"```(json)?\", \"\", raw_output).strip()\n",
    "\n",
    "        # Find the first JSON-like object in the response\n",
    "        json_match = re.search(r'\\{.*\\}', cleaned_output, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"No valid JSON found in response\")\n",
    "\n",
    "        json_str = json_match.group(0)\n",
    "        parsed = json.loads(json_str)\n",
    "\n",
    "        return parsed[\"corrected_query\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Could not parse LLM output properly.\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def enrich_prompt_with_aliases(prompt, alias_dict):\n",
    "    alias_info = \"\\n\".join([f\"{k} -> {v}\" for k, v in alias_dict.items()])\n",
    "    return f\"{prompt}\\n\\nColumn aliases:\\n{alias_info}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe560393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def format_tableschema(raw_input):\n",
    "\n",
    "    # Initialize a dictionary with list values\n",
    "    table_columns = defaultdict(set)\n",
    "\n",
    "    # Process each line\n",
    "    for line in raw_input.strip().splitlines():\n",
    "        table, column = line.strip().split('.')\n",
    "        table_columns[table].add(column)\n",
    "\n",
    "    # Convert sets to sorted lists\n",
    "    formatted_output = {table: sorted(list(columns)) for table, columns in table_columns.items()}\n",
    "\n",
    "    # Print result\n",
    "    # import json\n",
    "    # print(json.dumps(formatted_output, indent=4))\n",
    "\n",
    "\n",
    "    return formatted_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bca902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def formating_columns_for_input(column_list):\n",
    "    \"\"\"\n",
    "    Converts a list of 'table.column' strings into a dictionary grouped by table name.\n",
    "\n",
    "    Args:\n",
    "        column_list (list): List of strings in the format 'table.column'\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with table names as keys and lists of column names as values\n",
    "    \"\"\"\n",
    "    table_columns = defaultdict(list)\n",
    "    \n",
    "    for item in column_list:\n",
    "        if '.' in item:\n",
    "            table, column = item.split('.', 1)\n",
    "            table_columns[table].append(column)\n",
    "    \n",
    "    return dict(table_columns)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c88d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_s = formating_columns_for_input(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bfe0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "import sqlite3\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.sql_database.query import create_sql_query_chain\n",
    "\n",
    "def query_generation(given_prompt, db, llm, schema_dict):\n",
    "\n",
    "    db_path = \"data/grocery_sales.db\"\n",
    "\n",
    "    # Step 1: Correcting spelling mistakes or shortforms in the prompt using the schema\n",
    "    corrected_prompt = llm_correct_prompt(given_prompt, schema_dict, llm)\n",
    "\n",
    "    # Step 2: Loading the FAISS index and retrieving the most relevant schema entries\n",
    "    index = load_schema_index()\n",
    "    matches = search_schema(index, corrected_prompt)\n",
    "\n",
    "    # Formatting matched schema entries into table-wise structure\n",
    "    table_schema_formatted = formating_columns_for_input(matches)\n",
    "\n",
    "    # Step 3: Creating a custom prompt to instruct the LLM on how to write SQL queries\n",
    "    custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    *** You are an expert at writing SQL queries based on natural language questions.\n",
    "    There can be multiple tables in the database, and you can use any of them to answer the question. You can also join tables if needed. You can also use aggregate functions like COUNT, SUM, AVG, etc. to answer the question. You can also use GROUP BY and ORDER BY clauses if needed.\n",
    "    You have full liberty to use any SQL functions or clauses to answer the question. You can also use subqueries if needed. You can also use DISTINCT keyword if needed. You can also use WHERE clause to filter the results. You can also use HAVING clause to filter the results after aggregation. You can also use LIMIT clause to limit the number of results returned.\n",
    "    In short you can do anything to answer the question. ***\n",
    "\n",
    "    Use the following table schema:()\n",
    "    {table_info}\n",
    "\n",
    "    The user wants to retrieve up to {top_k} results, but **do not add a LIMIT clause unless explicitly instructed**.\n",
    "\n",
    "    Question: {input}\n",
    "\n",
    "    Return the SQL query in the following format:\n",
    "\n",
    "    SQLQuery: <your SQL query here>;\n",
    "    \"\"\")\n",
    "\n",
    "    # Step 4: Building the LangChain SQL query generation chain using the LLM and custom prompt\n",
    "    sql_chain = create_sql_query_chain(llm=llm, db=db, prompt=custom_prompt)\n",
    "\n",
    "    # Step 5: Sending the corrected prompt and schema to the chain to generate a query\n",
    "    write_query = sql_chain.invoke({\n",
    "        \"input\": corrected_prompt,\n",
    "        \"top_k\": 250,\n",
    "        \"table_info\": table_schema_formatted,\n",
    "        \"question\" : corrected_prompt\n",
    "    })\n",
    "\n",
    "    # Step 6: Extracting the SQL statement from the LLM's response\n",
    "    def extract_sql(query_response: str) -> str:\n",
    "        match = re.search(r\"SELECT .*?;\", query_response, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(0).strip() if match else \"\"\n",
    "\n",
    "    cleaned_query = extract_sql(write_query)\n",
    "    print(\"Formatted query:\", cleaned_query)\n",
    "\n",
    "    # Step 7: Running the generated SQL query on the database\n",
    "    result = db.run(cleaned_query)\n",
    "\n",
    "    # Step 8: Preparing a prompt to ask the LLM for a human-readable answer based on SQL output\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Given the following some instructions in *** ***,  user question, corresponding SQL query, and SQL result, answer the user question. \n",
    "        *** Don't sumarize the SQL result, until asked to do explicitly. ***\n",
    "        *** If there is any error in the SQL query, intepret it and answer the user accordingly without telling that there is any error in the system. ***\n",
    "        *** If there is no result or vague result then simple say no details found.***\n",
    "    Question: {question}    \n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    # Filling in the question, query, and result into the answer prompt\n",
    "    final_answer_prompt = answer_prompt.format(question=corrected_prompt, query=cleaned_query, result=result)\n",
    "\n",
    "    # Invoking the LLM to get the final natural language answer\n",
    "    response = llm.invoke(final_answer_prompt)\n",
    "\n",
    "    # Extracting and returning just the content part of the LLM response\n",
    "    content = response.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568a3b9",
   "metadata": {},
   "source": [
    "With sqlite server query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "import sqlite3\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.sql_database.query import create_sql_query_chain\n",
    "\n",
    "def query_generation_sql(given_prompt, db, llm, schema_dict):\n",
    "\n",
    "    db_path = \"data/grocery_sales.db\"\n",
    "    # Step 1: Correct column aliases\n",
    "    corrected_prompt = llm_correct_prompt(given_prompt, schema_dict, llm)\n",
    "\n",
    "    # Step 2: Retrieve relevant schema\n",
    "    index = load_schema_index()\n",
    "    matches = search_schema(index, corrected_prompt)\n",
    "\n",
    "\n",
    "    table_schema_formatted = formating_columns_for_input(matches)\n",
    "\n",
    "    # Step 3: Create PromptTemplate\n",
    "    custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    *** You are an expert at writing SQL queries based on natural language questions.\n",
    "    There can be multiple tables in the database, and you can use any of them to answer the question. You can also join tables if needed. You can also use aggregate functions like COUNT, SUM, AVG, etc. to answer the question. You can also use GROUP BY and ORDER BY clauses if needed.\n",
    "    You have full liberty to use any SQL functions or clauses to answer the question. You can also use subqueries if needed. You can also use DISTINCT keyword if needed. You can also use WHERE clause to filter the results. You can also use HAVING clause to filter the results after aggregation. You can also use LIMIT clause to limit the number of results returned.\n",
    "    In short you can do anything to answer the question. ***\n",
    "\n",
    "    Use the following table schema:()\n",
    "    {table_info}\n",
    "\n",
    "    The user wants to retrieve up to {top_k} results, but **do not add a LIMIT clause unless explicitly instructed**.\n",
    "\n",
    "    Question: {input}\n",
    "\n",
    "    Return the SQL query in the following format:\n",
    "\n",
    "    SQLQuery: <your SQL query here>;\n",
    "    \"\"\")\n",
    "\n",
    "    # Step 4: Build the SQL query generation chain\n",
    "    sql_chain = create_sql_query_chain(llm=llm, db=db, prompt=custom_prompt)\n",
    "\n",
    "    # Step 5: Invoke the chain\n",
    "    write_query = sql_chain.invoke({\n",
    "        \"input\": corrected_prompt,\n",
    "        \"top_k\": 250,\n",
    "        \"table_info\": table_schema_formatted,\n",
    "        \"question\" : corrected_prompt\n",
    "    })\n",
    "\n",
    "    # Step 6: Extract SQL\n",
    "    def extract_sql(query_response: str) -> str:\n",
    "        match = re.search(r\"SELECT .*?;\", query_response, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(0).strip() if match else \"\"\n",
    "\n",
    "    cleaned_query = extract_sql(write_query)\n",
    "    print(\"Formatted query:\", cleaned_query)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def execute_sql_query(query: str, db_path: str) -> list:\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            column_names = [description[0] for description in cursor.description]\n",
    "            conn.close()\n",
    "            return [dict(zip(column_names, row)) for row in result]\n",
    "        except Exception as e:\n",
    "            print(f\"Query execution failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    result = execute_sql_query(cleaned_query, db_path)\n",
    "\n",
    "\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Given the following some instructions in *** ***,  user question, corresponding SQL query, and SQL result, answer the user question. \n",
    "        *** Don't sumarize the SQL result, until asked to do explicitly. ***\n",
    "        *** If there is any error in the SQL query, intepret it and answer the user accordingly without telling that there is any error in the system. ***\n",
    "        *** If there is no result or vague result then simple say no details found.***\n",
    "    Question: {question}    \n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    final_answer_prompt = answer_prompt.format(question=corrected_prompt, query = cleaned_query, result = result)\n",
    "    response = llm.invoke(final_answer_prompt)\n",
    "\n",
    "    # Extract the text content from the AIMessage\n",
    "    content = response.content\n",
    "\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "groc_db_path = rf\"D:\\i2e Internship projects\\New ChatBot\\data\\grocery_sales.db\"     # Grocery Database Path\n",
    "groc_db = SQLDatabase.from_uri(f\"sqlite:///{groc_db_path}\")        # Loading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33269cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_prompt  = \"Which prdct has highest sal\"    # Giving the prompt  \n",
    "out = query_generation(given_prompt, groc_db, llm, schema_grocery_sales)  # Calling the function "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
