{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc9b0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema exported to JSON with column names and data types.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Path to your SQLite DB file\n",
    "db_path = \"data/grocery_sales.db\"\n",
    "db_name = os.path.splitext(os.path.basename(db_path))[0]\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get all user-defined tables\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT name FROM sqlite_master\n",
    "    WHERE type='table' AND name NOT LIKE 'sqlite_%';\n",
    "\"\"\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Build schema dictionary with column names and data types\n",
    "schema_info = {}\n",
    "\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    cursor.execute(f\"PRAGMA table_info('{table_name}')\")\n",
    "    columns = cursor.fetchall()\n",
    "    column_info = {col[1]: col[2] for col in columns}  # {column_name: data_type}\n",
    "    schema_info[table_name] = column_info\n",
    "\n",
    "# Convert to JSON\n",
    "schema_json = json.dumps(schema_info, indent=2)\n",
    "\n",
    "json_path = db_name + \".json\"\n",
    "\n",
    "# Optional: write to file\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    f.write(schema_json)\n",
    "\n",
    "# Cleanup\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Schema exported to JSON with column names and data types.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8cff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91878\\AppData\\Local\\Temp\\ipykernel_19028\\2156166289.py:14: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "groq_api_key = \"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\"\n",
    "groq_api_base = \"https://api.groq.com/openai/v1\"\n",
    "model_name = \"llama3-70b-8192\"\n",
    "llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "    temperature=0.0,\n",
    "    openai_api_key=groq_api_key,\n",
    "    openai_api_base=groq_api_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df752972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for file handling, serialization, vector search, and embeddings\n",
    "import os\n",
    "import pickle\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Setting constants for the schema index directory and embedding model to use\n",
    "SCHEMA_INDEX_DIR = \"schema_index\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Build or Load Schema Embeddings\n",
    "# -----------------------------------------------\n",
    "\n",
    "def build_schema_index(schema_dict, save_path=SCHEMA_INDEX_DIR):\n",
    "    # Initializing the Hugging Face sentence embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    \n",
    "    # Flattening the schema into \"table.column\" format for embedding\n",
    "    schema_lines = [f\"{table}.{col}\" for table, cols in schema_dict.items() for col in cols]\n",
    "    # Wrapping each schema line into a Document object\n",
    "    documents = [Document(page_content=line) for line in schema_lines]\n",
    "\n",
    "    # Creating a FAISS vector index from the embedded documents\n",
    "    db = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "    # Creating the directory if it doesn't exist and saving the FAISS index\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    db.save_local(folder_path=save_path)\n",
    "    print(f\"Schema index saved to: {save_path}\")\n",
    "\n",
    "def load_schema_index(load_path=SCHEMA_INDEX_DIR):\n",
    "    # Initializing the same embedding model to match the one used in indexing\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    # Loading the FAISS index from the local directory\n",
    "    return FAISS.load_local(\n",
    "        folder_path=load_path,\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True  # Needed if using pickle under the hood\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Query Similar Schema Entries\n",
    "# -----------------------------------------------\n",
    "\n",
    "def search_schema(index, query, top_k=5):\n",
    "    # Performing a similarity search on the index for the given query\n",
    "    results = index.similarity_search(query, k=top_k)\n",
    "    # Returning the matched schema entries as plain text\n",
    "    return [doc.page_content for doc in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the 'grocery_sales.json' file in read mode\n",
    "with open('grocery_sales.json', 'r') as file:\n",
    "    # Loading the JSON content into a Python dictionary\n",
    "    schema_grocery_sales = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91878\\AppData\\Local\\Temp\\ipykernel_19028\\2224219357.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\91878\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91878\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Schema index saved to: schema_index\n"
     ]
    }
   ],
   "source": [
    "# Creating the schema of the DataBase and First-time build (comment this after first run)\n",
    "build_schema_index(schema_grocery_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e69606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def llm_correct_prompt(query: str, alias_dict: dict, llm) -> str:\n",
    "    \"\"\"\n",
    "    Uses LLM to correct typos/shortforms in the query using alias_dict/schema.\n",
    "    Returns the corrected query string only.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting the alias dictionary to a pretty-printed JSON string for context\n",
    "    db_schema_str = json.dumps(alias_dict, indent=2)\n",
    "\n",
    "    # Defining the prompt template for the LLM to correct the user query\n",
    "    prompt_temp = PromptTemplate.from_template(\"\"\"\n",
    "You are a Data Assistant and SQL expert. Your task is to correct user prompts for spelling mistakes or shortforms based on the database schema.\n",
    "\n",
    "The schema is in the format:\n",
    "\"Table name\": {{\n",
    "  \"Column name\": \"datatype\",\n",
    "  \"Column name\": \"datatype\"    \n",
    "}}\n",
    "\n",
    "User prompt:\n",
    "{query}\n",
    "\n",
    "Database schema:\n",
    "{db_schema}\n",
    "\n",
    "Return your response in **only** this JSON format:\n",
    "{{ \"corrected_query\": \"...\" }}\n",
    "\"\"\")\n",
    "\n",
    "    # Filling in the query and schema into the prompt\n",
    "    prompt = prompt_temp.format(query=query, db_schema=db_schema_str)\n",
    "    \n",
    "    # Sending the prompt to the LLM and capturing its response\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Stripping extra whitespace from the LLM's raw output\n",
    "    raw_output = response.content.strip()\n",
    "    print(\"üîç Raw LLM Output:\\n\", raw_output)\n",
    "\n",
    "    try:\n",
    "        # Removing markdown code block markers if present\n",
    "        cleaned_output = re.sub(r\"```(json)?\", \"\", raw_output).strip()\n",
    "\n",
    "        # Extracting the JSON content from the response\n",
    "        json_match = re.search(r'\\{.*\\}', cleaned_output, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"No valid JSON found in response\")\n",
    "\n",
    "        json_str = json_match.group(0)\n",
    "        parsed = json.loads(json_str)\n",
    "\n",
    "        # Returning only the corrected query from the parsed JSON\n",
    "        return parsed[\"corrected_query\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handling cases where the LLM's output can't be parsed\n",
    "        print(\"‚ùå Could not parse LLM output properly.\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be13b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def format_tableschema(raw_input):\n",
    "\n",
    "    # Initialize a dictionary with list values\n",
    "    table_columns = defaultdict(set)\n",
    "\n",
    "    # Process each line\n",
    "    for line in raw_input.strip().splitlines():\n",
    "        table, column = line.strip().split('.')\n",
    "        table_columns[table].add(column)\n",
    "\n",
    "    # Convert sets to sorted lists\n",
    "    formatted_output = {table: sorted(list(columns)) for table, columns in table_columns.items()}\n",
    "\n",
    "    # Print result\n",
    "    # import json\n",
    "    # print(json.dumps(formatted_output, indent=4))\n",
    "\n",
    "\n",
    "    return formatted_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def formating_columns_for_input(column_list):\n",
    "    \"\"\"\n",
    "    Converts a list of 'table.column' strings into a dictionary grouped by table name.\n",
    "\n",
    "    Args:\n",
    "        column_list (list): List of strings in the format 'table.column'\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with table names as keys and lists of column names as values\n",
    "    \"\"\"\n",
    "\n",
    "    # Initializing a dictionary where each key maps to a list of columns\n",
    "    table_columns = defaultdict(list)\n",
    "    \n",
    "    # Iterating through each table.column string in the list\n",
    "    for item in column_list:\n",
    "        # Checking that the string is properly formatted with a dot separator\n",
    "        if '.' in item:\n",
    "            # Splitting into table name and column name\n",
    "            table, column = item.split('.', 1)\n",
    "            # Grouping the column under the appropriate table\n",
    "            table_columns[table].append(column)\n",
    "    \n",
    "    # Converting the defaultdict to a regular dictionary and returning it\n",
    "    return dict(table_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "import sqlite3\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.sql_database.query import create_sql_query_chain\n",
    "\n",
    "def query_generation(given_prompt, db, llm, schema_dict):\n",
    "\n",
    "    db_path = \"data/grocery_sales.db\"\n",
    "\n",
    "    # Step 1: Correcting spelling mistakes or shortforms in the prompt using the schema\n",
    "    corrected_prompt = llm_correct_prompt(given_prompt, schema_dict, llm)\n",
    "\n",
    "    # Step 2: Loading the FAISS index and retrieving the most relevant schema entries\n",
    "    index = load_schema_index()\n",
    "    matches = search_schema(index, corrected_prompt)\n",
    "\n",
    "    # Formatting matched schema entries into table-wise structure\n",
    "    table_schema_formatted = formating_columns_for_input(matches)\n",
    "\n",
    "    # Step 3: Creating a custom prompt to instruct the LLM on how to write SQL queries\n",
    "    custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    *** You are an expert at writing SQL queries based on natural language questions.\n",
    "    There can be multiple tables in the database, and you can use any of them to answer the question. You can also join tables if needed. You can also use aggregate functions like COUNT, SUM, AVG, etc. to answer the question. You can also use GROUP BY and ORDER BY clauses if needed.\n",
    "    You have full liberty to use any SQL functions or clauses to answer the question. You can also use subqueries if needed. You can also use DISTINCT keyword if needed. You can also use WHERE clause to filter the results. You can also use HAVING clause to filter the results after aggregation. You can also use LIMIT clause to limit the number of results returned.\n",
    "    In short you can do anything to answer the question. ***\n",
    "\n",
    "    Use the following table schema:()\n",
    "    {table_info}\n",
    "\n",
    "    The user wants to retrieve up to {top_k} results, but **do not add a LIMIT clause unless explicitly instructed**.\n",
    "\n",
    "    Question: {input}\n",
    "\n",
    "    Return the SQL query in the following format:\n",
    "\n",
    "    SQLQuery: <your SQL query here>;\n",
    "    \"\"\")\n",
    "\n",
    "    # Step 4: Building the LangChain SQL query generation chain using the LLM and custom prompt\n",
    "    sql_chain = create_sql_query_chain(llm=llm, db=db, prompt=custom_prompt)\n",
    "\n",
    "    # Step 5: Sending the corrected prompt and schema to the chain to generate a query\n",
    "    write_query = sql_chain.invoke({\n",
    "        \"input\": corrected_prompt,\n",
    "        \"top_k\": 250,\n",
    "        \"table_info\": table_schema_formatted,\n",
    "        \"question\" : corrected_prompt\n",
    "    })\n",
    "\n",
    "    # Step 6: Extracting the SQL statement from the LLM's response\n",
    "    def extract_sql(query_response: str) -> str:\n",
    "        match = re.search(r\"SELECT .*?;\", query_response, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(0).strip() if match else \"\"\n",
    "\n",
    "    cleaned_query = extract_sql(write_query)\n",
    "    print(\"Formatted query:\", cleaned_query)\n",
    "\n",
    "    # Step 7: Running the generated SQL query on the database\n",
    "    result = db.run(cleaned_query)\n",
    "\n",
    "    # Step 8: Preparing a prompt to ask the LLM for a human-readable answer based on SQL output\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Given the following some instructions in *** ***,  user question, corresponding SQL query, and SQL result, answer the user question. \n",
    "        *** Don't sumarize the SQL result, until asked to do explicitly. ***\n",
    "        *** If there is any error in the SQL query, intepret it and answer the user accordingly without telling that there is any error in the system. ***\n",
    "        *** If there is no result or vague result then simple say no details found.***\n",
    "    Question: {question}    \n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    # Filling in the question, query, and result into the answer prompt\n",
    "    final_answer_prompt = answer_prompt.format(question=corrected_prompt, query=cleaned_query, result=result)\n",
    "\n",
    "    # Invoking the LLM to get the final natural language answer\n",
    "    response = llm.invoke(final_answer_prompt)\n",
    "\n",
    "    # Extracting and returning just the content part of the LLM response\n",
    "    content = response.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eccb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "groc_db_path = rf\"D:\\i2e Internship projects\\New ChatBot\\data\\grocery_sales.db\"     # Grocery Database Path\n",
    "groc_db = SQLDatabase.from_uri(f\"sqlite:///{groc_db_path}\")        # Loading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw LLM Output:\n",
      " { \"corrected_query\": \"Which product has the highest sale\" }\n",
      "Formatted query: SELECT p.ProductName \n",
      "FROM sales s \n",
      "JOIN products p ON s.ProductID = p.ProductID \n",
      "GROUP BY p.ProductName \n",
      "ORDER BY SUM(TotalPrice) DESC;\n"
     ]
    }
   ],
   "source": [
    "given_prompt  = \"Which prdct has highest sal\"    # Giving the prompt  \n",
    "out = query_generation(given_prompt, groc_db, llm, schema_grocery_sales)  # Calling the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product with the highest sale is Zucchini - Yellow.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1604f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
